{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "680117a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf759fe6",
   "metadata": {},
   "source": [
    "# Simple Recommender Model\n",
    "The architecture includes embedding layers that's utilized for users and items. It computes the dot product of the user and item embeddings to get the predicted rating. The actication function used is a sigmoid activation function to the dot product result to map it to a probability.\n",
    "\n",
    "### Complexity:\n",
    "The model is straightforward with fewer parameters and no additional dense layers. But it is limited to capturing linear interactions between users and items. \n",
    "\n",
    "#### Please scroll below to find the enhanced version of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65153582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MovieLens dataset\n",
    "data_url = 'http://files.grouplens.org/datasets/movielens/ml-100k/u.data'\n",
    "column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv(data_url, sep='\\t', names=column_names)\n",
    "\n",
    "# Encode user_id and item_id\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "df['user_id'] = user_encoder.fit_transform(df['user_id'])\n",
    "df['item_id'] = item_encoder.fit_transform(df['item_id'])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'user_id': train_df['user_id'], 'item_id': train_df['item_id']}, train_df['rating']))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(({'user_id': test_df['user_id'], 'item_id': test_df['item_id']}, test_df['rating']))\n",
    "\n",
    "# Batch and prefetch the data\n",
    "train_dataset = train_dataset.shuffle(len(train_df)).batch(256).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(256).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c80c2261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "313/313 [==============================] - 9s 3ms/step - loss: 0.6325 - accuracy: 0.0096 - val_loss: 0.2732 - val_accuracy: 0.0127\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 7s 2ms/step - loss: -3.8118 - accuracy: 0.0096 - val_loss: -10.9203 - val_accuracy: 0.0253\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 5s 2ms/step - loss: -24.1496 - accuracy: 0.0032 - val_loss: -39.8874 - val_accuracy: 0.0127\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 5s 2ms/step - loss: -60.9219 - accuracy: 0.0160 - val_loss: -83.1760 - val_accuracy: 0.0127\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 3s 2ms/step - loss: -110.5768 - accuracy: 0.0096 - val_loss: -137.8652 - val_accuracy: 0.0127\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 2s 2ms/step - loss: -171.0253 - accuracy: 0.0064 - val_loss: -202.5764 - val_accuracy: 0.0127\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 2s 2ms/step - loss: -241.0948 - accuracy: 0.0064 - val_loss: -276.3478 - val_accuracy: 0.0127\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 2s 2ms/step - loss: -320.0395 - accuracy: 0.0064 - val_loss: -358.6382 - val_accuracy: 0.0127\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 2s 3ms/step - loss: -407.3600 - accuracy: 0.0096 - val_loss: -448.9394 - val_accuracy: 0.0127\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 3s 3ms/step - loss: -502.6013 - accuracy: 0.0032 - val_loss: -546.8516 - val_accuracy: 0.0127\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 3s 2ms/step - loss: -605.3267 - accuracy: 0.0064 - val_loss: -652.0205 - val_accuracy: 0.0127\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 2s 2ms/step - loss: -715.3000 - accuracy: 0.0064 - val_loss: -764.2544 - val_accuracy: 0.0127\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 2s 3ms/step - loss: -832.3806 - accuracy: 0.0064 - val_loss: -883.4251 - val_accuracy: 0.0127\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 2s 3ms/step - loss: -956.4354 - accuracy: 0.0192 - val_loss: -1009.4053 - val_accuracy: 0.0127\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 1s 2ms/step - loss: -1087.2498 - accuracy: 0.0096 - val_loss: -1142.0500 - val_accuracy: 0.0127\n",
      "79/79 [==============================] - 0s 2ms/step - loss: -1142.0500 - accuracy: 0.0127\n",
      "Test Loss: -1142.050048828125\n",
      "Test Accuracy: 0.012658228166401386\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the model\n",
    "class SimpleRecommenderNet(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_items, embedding_size=50, **kwargs):\n",
    "        super(SimpleRecommenderNet, self).__init__(**kwargs)\n",
    "        self.user_embedding = layers.Embedding(num_users, embedding_size, embeddings_initializer='he_normal', embeddings_regularizer=tf.keras.regularizers.l2(1e-6))\n",
    "        self.item_embedding = layers.Embedding(num_items, embedding_size, embeddings_initializer='he_normal', embeddings_regularizer=tf.keras.regularizers.l2(1e-6))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs['user_id'])\n",
    "        item_vector = self.item_embedding(inputs['item_id'])\n",
    "        dot_user_item = tf.reduce_sum(user_vector * item_vector, axis=1)\n",
    "        return tf.nn.sigmoid(dot_user_item)\n",
    "\n",
    "num_users = df['user_id'].nunique()\n",
    "num_items = df['item_id'].nunique()\n",
    "model = SimpleRecommenderNet(num_users, num_items)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=15, validation_data=test_dataset)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b51d5",
   "metadata": {},
   "source": [
    "# Enhanced Recommender Model\n",
    "\n",
    "The same architecture was used by using embedding layers for users and time. However, this model also includes concatenation and multiple dense layers using ReLU activation to make better predictions. The output layer uses a final dense layer with sigmoid activation to predict the rating.\n",
    "\n",
    "### Complexity:\n",
    "The enhanced version includes additional dense layers, making it more powerful with a higher capacity to learn complex patterns. And it can capture non-linear interactions between users and items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5863daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MovieLens dataset\n",
    "data_url = 'http://files.grouplens.org/datasets/movielens/ml-100k/u.data'\n",
    "column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv(data_url, sep='\\t', names=column_names)\n",
    "\n",
    "# Encode user_id and item_id\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "df['user_id'] = user_encoder.fit_transform(df['user_id'])\n",
    "df['item_id'] = item_encoder.fit_transform(df['item_id'])\n",
    "\n",
    "# Normalize ratings to 0 or 1\n",
    "df['rating'] = df['rating'].apply(lambda x: 1 if x >= 3 else 0)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'user_id': train_df['user_id'].values, 'item_id': train_df['item_id'].values}, train_df['rating'].values))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(({'user_id': test_df['user_id'].values, 'item_id': test_df['item_id'].values}, test_df['rating'].values))\n",
    "\n",
    "# Batch and prefetch the data\n",
    "train_dataset = train_dataset.shuffle(len(train_df)).batch(256).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(256).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a279c10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 4s 4ms/step - loss: 0.4149 - accuracy: 0.8307 - val_loss: 0.3756 - val_accuracy: 0.8425\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 5s 4ms/step - loss: 0.3635 - accuracy: 0.8475 - val_loss: 0.3738 - val_accuracy: 0.8451\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 0.3531 - accuracy: 0.8537 - val_loss: 0.3705 - val_accuracy: 0.8464\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3432 - accuracy: 0.8587 - val_loss: 0.3724 - val_accuracy: 0.8465\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 5s 4ms/step - loss: 0.3317 - accuracy: 0.8637 - val_loss: 0.3759 - val_accuracy: 0.8456\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 2s 3ms/step - loss: 0.3153 - accuracy: 0.8700 - val_loss: 0.3837 - val_accuracy: 0.8398\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 2s 3ms/step - loss: 0.2928 - accuracy: 0.8805 - val_loss: 0.3955 - val_accuracy: 0.8413\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 0.2636 - accuracy: 0.8930 - val_loss: 0.4180 - val_accuracy: 0.8338\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 0.2318 - accuracy: 0.9091 - val_loss: 0.4478 - val_accuracy: 0.8253\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 0.2001 - accuracy: 0.9223 - val_loss: 0.4934 - val_accuracy: 0.8314\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.4934 - accuracy: 0.8314\n",
      "Test Loss: 0.49344485998153687\n",
      "Test Accuracy: 0.8314499855041504\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the model\n",
    "class EnhancedRecommenderNet(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_items, embedding_size=50, **kwargs):\n",
    "        super(EnhancedRecommenderNet, self).__init__(**kwargs)\n",
    "        self.user_embedding = layers.Embedding(num_users, embedding_size, embeddings_initializer='he_normal', embeddings_regularizer=tf.keras.regularizers.l2(1e-6))\n",
    "        self.item_embedding = layers.Embedding(num_items, embedding_size, embeddings_initializer='he_normal', embeddings_regularizer=tf.keras.regularizers.l2(1e-6))\n",
    "        self.dense_1 = layers.Dense(128, activation='relu')\n",
    "        self.dense_2 = layers.Dense(64, activation='relu')\n",
    "        self.output_layer = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs['user_id'])\n",
    "        item_vector = self.item_embedding(inputs['item_id'])\n",
    "        concat = tf.concat([user_vector, item_vector], axis=-1)\n",
    "        x = self.dense_1(concat)\n",
    "        x = self.dense_2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "num_users = df['user_id'].nunique()\n",
    "num_items = df['item_id'].nunique()\n",
    "model = EnhancedRecommenderNet(num_users, num_items)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd0394e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
